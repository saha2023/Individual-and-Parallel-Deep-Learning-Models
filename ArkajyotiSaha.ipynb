{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjonSkFWGc6w"
      },
      "source": [
        "## Single Model Code\n",
        "This script focuses on training and evaluating individual models (e.g., VGG16, ResNet18, GoogLeNet) on datasets like MNIST and CIFAR10, using a two-phase approach: feature extraction and fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXTyejx5srYR",
        "outputId": "b9c8d738-b008-482d-e67f-20ed8e523cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchviz)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchviz)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchviz)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchviz)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchviz)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchviz)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchviz-0.0.3\n"
          ]
        }
      ],
      "source": [
        "pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLg93ZIOsvDQ",
        "outputId": "b233fc81-48e5-422e-d8a0-6a9f4eb84b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4PCqYymsyxJ",
        "outputId": "66019e44-192a-48c2-f018-56eb9c34100e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Collecting numpy<2.0 (from captum)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
            "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, captum\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed captum-0.8.0 numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZcol0Fnrt6e",
        "outputId": "a85a6213-9b93-43db-a0ed-9ac12f3cf9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.16MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 12.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: MNIST\n",
            "  Training samples: 60000\n",
            "  Validation/Test samples: 10000\n",
            "  Number of classes: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Starting Experiment: vgg16_MNIST ====================\n",
            "Loading model: vgg16 (pretrained=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 230MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Replaced final layer. Original input features: 4096, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.5232 Acc: 0.8288\n",
            "Val Loss: 0.2312 Acc: 0.9348\n",
            "  New best validation accuracy: 0.9348\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.4267 Acc: 0.8614\n",
            "Val Loss: 0.2012 Acc: 0.9413\n",
            "  New best validation accuracy: 0.9413\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.4181 Acc: 0.8623\n",
            "Val Loss: 0.1963 Acc: 0.9395\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.4198 Acc: 0.8670\n",
            "Val Loss: 0.1856 Acc: 0.9476\n",
            "  New best validation accuracy: 0.9476\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.4231 Acc: 0.8677\n",
            "Val Loss: 0.1849 Acc: 0.9455\n",
            "\n",
            "Training complete in 4m 60s\n",
            "Best val Acc: 0.947600\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 0.1007 Acc: 0.9692\n",
            "Val Loss: 0.0366 Acc: 0.9873\n",
            "  New best validation accuracy: 0.9873\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.0354 Acc: 0.9892\n",
            "Val Loss: 0.0267 Acc: 0.9925\n",
            "  New best validation accuracy: 0.9925\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.0244 Acc: 0.9922\n",
            "Val Loss: 0.0304 Acc: 0.9904\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.0176 Acc: 0.9946\n",
            "Val Loss: 0.0275 Acc: 0.9920\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.0125 Acc: 0.9960\n",
            "Val Loss: 0.0280 Acc: 0.9928\n",
            "  New best validation accuracy: 0.9928\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0110 Acc: 0.9965\n",
            "Val Loss: 0.0209 Acc: 0.9938\n",
            "  New best validation accuracy: 0.9938\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.0085 Acc: 0.9973\n",
            "Val Loss: 0.0256 Acc: 0.9932\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0070 Acc: 0.9977\n",
            "Val Loss: 0.0203 Acc: 0.9947\n",
            "  New best validation accuracy: 0.9947\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0062 Acc: 0.9983\n",
            "Val Loss: 0.0231 Acc: 0.9935\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0062 Acc: 0.9978\n",
            "Val Loss: 0.0291 Acc: 0.9934\n",
            "\n",
            "Training complete in 22m 11s\n",
            "Best val Acc: 0.994700\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for vgg16_MNIST ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9947\n",
            "\n",
            "==================== Starting Experiment: resnet18_MNIST ====================\n",
            "Loading model: resnet18 (pretrained=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Replaced final layer. Original input features: 512, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.3570 Acc: 0.9042\n",
            "Val Loss: 0.1621 Acc: 0.9521\n",
            "  New best validation accuracy: 0.9521\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.1756 Acc: 0.9470\n",
            "Val Loss: 0.1358 Acc: 0.9558\n",
            "  New best validation accuracy: 0.9558\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.1485 Acc: 0.9537\n",
            "Val Loss: 0.1177 Acc: 0.9617\n",
            "  New best validation accuracy: 0.9617\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.1374 Acc: 0.9573\n",
            "Val Loss: 0.1135 Acc: 0.9632\n",
            "  New best validation accuracy: 0.9632\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.1302 Acc: 0.9584\n",
            "Val Loss: 0.1049 Acc: 0.9675\n",
            "  New best validation accuracy: 0.9675\n",
            "\n",
            "Training complete in 4m 58s\n",
            "Best val Acc: 0.967500\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 0.0595 Acc: 0.9815\n",
            "Val Loss: 0.0316 Acc: 0.9918\n",
            "  New best validation accuracy: 0.9918\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.0208 Acc: 0.9932\n",
            "Val Loss: 0.0247 Acc: 0.9928\n",
            "  New best validation accuracy: 0.9928\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.0132 Acc: 0.9959\n",
            "Val Loss: 0.0396 Acc: 0.9894\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.0102 Acc: 0.9966\n",
            "Val Loss: 0.0253 Acc: 0.9932\n",
            "  New best validation accuracy: 0.9932\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.0083 Acc: 0.9971\n",
            "Val Loss: 0.0213 Acc: 0.9945\n",
            "  New best validation accuracy: 0.9945\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0075 Acc: 0.9977\n",
            "Val Loss: 0.0222 Acc: 0.9942\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.0054 Acc: 0.9982\n",
            "Val Loss: 0.0203 Acc: 0.9949\n",
            "  New best validation accuracy: 0.9949\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0057 Acc: 0.9981\n",
            "Val Loss: 0.0260 Acc: 0.9938\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0043 Acc: 0.9986\n",
            "Val Loss: 0.0297 Acc: 0.9941\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0053 Acc: 0.9982\n",
            "Val Loss: 0.0419 Acc: 0.9926\n",
            "\n",
            "Training complete in 10m 1s\n",
            "Best val Acc: 0.994900\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for resnet18_MNIST ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9949\n",
            "\n",
            "==================== Starting Experiment: googlenet_MNIST ====================\n",
            "Loading model: googlenet (pretrained=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 136MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Replaced final layer. Original input features: 1024, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n",
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7785 Acc: 0.8650\n",
            "Val Loss: 0.2253 Acc: 0.9392\n",
            "  New best validation accuracy: 0.9392\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.4650 Acc: 0.9132\n",
            "Val Loss: 0.1778 Acc: 0.9475\n",
            "  New best validation accuracy: 0.9475\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.4293 Acc: 0.9193\n",
            "Val Loss: 0.1690 Acc: 0.9466\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.4177 Acc: 0.9213\n",
            "Val Loss: 0.1520 Acc: 0.9526\n",
            "  New best validation accuracy: 0.9526\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.4047 Acc: 0.9249\n",
            "Val Loss: 0.1570 Acc: 0.9510\n",
            "\n",
            "Training complete in 4m 56s\n",
            "Best val Acc: 0.952600\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 0.1869 Acc: 0.9717\n",
            "Val Loss: 0.0336 Acc: 0.9898\n",
            "  New best validation accuracy: 0.9898\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.0883 Acc: 0.9889\n",
            "Val Loss: 0.0274 Acc: 0.9916\n",
            "  New best validation accuracy: 0.9916\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.0572 Acc: 0.9929\n",
            "Val Loss: 0.0268 Acc: 0.9921\n",
            "  New best validation accuracy: 0.9921\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.0434 Acc: 0.9957\n",
            "Val Loss: 0.0231 Acc: 0.9936\n",
            "  New best validation accuracy: 0.9936\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.0356 Acc: 0.9964\n",
            "Val Loss: 0.0260 Acc: 0.9935\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0279 Acc: 0.9978\n",
            "Val Loss: 0.0251 Acc: 0.9942\n",
            "  New best validation accuracy: 0.9942\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.0239 Acc: 0.9980\n",
            "Val Loss: 0.0214 Acc: 0.9943\n",
            "  New best validation accuracy: 0.9943\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0204 Acc: 0.9985\n",
            "Val Loss: 0.0250 Acc: 0.9941\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0201 Acc: 0.9980\n",
            "Val Loss: 0.0229 Acc: 0.9944\n",
            "  New best validation accuracy: 0.9944\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0169 Acc: 0.9989\n",
            "Val Loss: 0.0245 Acc: 0.9944\n",
            "\n",
            "Training complete in 15m 17s\n",
            "Best val Acc: 0.994400\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for googlenet_MNIST ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 80.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: CIFAR10\n",
            "  Training samples: 50000\n",
            "  Validation/Test samples: 10000\n",
            "  Number of classes: 10\n",
            "\n",
            "==================== Starting Experiment: vgg16_CIFAR10 ====================\n",
            "Loading model: vgg16 (pretrained=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Replaced final layer. Original input features: 4096, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.7287 Acc: 0.7490\n",
            "Val Loss: 0.6120 Acc: 0.7882\n",
            "  New best validation accuracy: 0.7882\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.7091 Acc: 0.7670\n",
            "Val Loss: 0.5501 Acc: 0.8134\n",
            "  New best validation accuracy: 0.8134\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.7203 Acc: 0.7684\n",
            "Val Loss: 0.5197 Acc: 0.8232\n",
            "  New best validation accuracy: 0.8232\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.7237 Acc: 0.7709\n",
            "Val Loss: 0.5202 Acc: 0.8255\n",
            "  New best validation accuracy: 0.8255\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.7320 Acc: 0.7702\n",
            "Val Loss: 0.5414 Acc: 0.8184\n",
            "\n",
            "Training complete in 4m 39s\n",
            "Best val Acc: 0.825500\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 0.4249 Acc: 0.8557\n",
            "Val Loss: 0.3148 Acc: 0.8901\n",
            "  New best validation accuracy: 0.8901\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.2784 Acc: 0.9040\n",
            "Val Loss: 0.2500 Acc: 0.9145\n",
            "  New best validation accuracy: 0.9145\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.2190 Acc: 0.9245\n",
            "Val Loss: 0.2355 Acc: 0.9199\n",
            "  New best validation accuracy: 0.9199\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.1800 Acc: 0.9380\n",
            "Val Loss: 0.2577 Acc: 0.9144\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.1478 Acc: 0.9487\n",
            "Val Loss: 0.2121 Acc: 0.9280\n",
            "  New best validation accuracy: 0.9280\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.1232 Acc: 0.9562\n",
            "Val Loss: 0.2210 Acc: 0.9329\n",
            "  New best validation accuracy: 0.9329\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.1068 Acc: 0.9624\n",
            "Val Loss: 0.2221 Acc: 0.9309\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0865 Acc: 0.9705\n",
            "Val Loss: 0.2324 Acc: 0.9283\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0765 Acc: 0.9731\n",
            "Val Loss: 0.2270 Acc: 0.9340\n",
            "  New best validation accuracy: 0.9340\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0653 Acc: 0.9780\n",
            "Val Loss: 0.2395 Acc: 0.9341\n",
            "  New best validation accuracy: 0.9341\n",
            "\n",
            "Training complete in 18m 42s\n",
            "Best val Acc: 0.934100\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for vgg16_CIFAR10 ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9341\n",
            "\n",
            "==================== Starting Experiment: resnet18_CIFAR10 ====================\n",
            "Loading model: resnet18 (pretrained=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Replaced final layer. Original input features: 512, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.7911 Acc: 0.7412\n",
            "Val Loss: 0.6185 Acc: 0.7887\n",
            "  New best validation accuracy: 0.7887\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.6294 Acc: 0.7832\n",
            "Val Loss: 0.5985 Acc: 0.7962\n",
            "  New best validation accuracy: 0.7962\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.6107 Acc: 0.7890\n",
            "Val Loss: 0.6331 Acc: 0.7833\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.6036 Acc: 0.7921\n",
            "Val Loss: 0.6009 Acc: 0.7958\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.5934 Acc: 0.7963\n",
            "Val Loss: 0.5708 Acc: 0.8079\n",
            "  New best validation accuracy: 0.8079\n",
            "\n",
            "Training complete in 4m 39s\n",
            "Best val Acc: 0.807900\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 0.3810 Acc: 0.8689\n",
            "Val Loss: 0.2498 Acc: 0.9150\n",
            "  New best validation accuracy: 0.9150\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.2371 Acc: 0.9191\n",
            "Val Loss: 0.2213 Acc: 0.9265\n",
            "  New best validation accuracy: 0.9265\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.1775 Acc: 0.9387\n",
            "Val Loss: 0.1934 Acc: 0.9368\n",
            "  New best validation accuracy: 0.9368\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.1399 Acc: 0.9509\n",
            "Val Loss: 0.1811 Acc: 0.9408\n",
            "  New best validation accuracy: 0.9408\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.1099 Acc: 0.9616\n",
            "Val Loss: 0.2021 Acc: 0.9351\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.0891 Acc: 0.9688\n",
            "Val Loss: 0.1741 Acc: 0.9479\n",
            "  New best validation accuracy: 0.9479\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.0741 Acc: 0.9741\n",
            "Val Loss: 0.1879 Acc: 0.9431\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.0613 Acc: 0.9783\n",
            "Val Loss: 0.1832 Acc: 0.9487\n",
            "  New best validation accuracy: 0.9487\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.0498 Acc: 0.9824\n",
            "Val Loss: 0.1987 Acc: 0.9468\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.0396 Acc: 0.9861\n",
            "Val Loss: 0.2003 Acc: 0.9475\n",
            "\n",
            "Training complete in 9m 21s\n",
            "Best val Acc: 0.948700\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for resnet18_CIFAR10 ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9487\n",
            "\n",
            "==================== Starting Experiment: googlenet_CIFAR10 ====================\n",
            "Loading model: googlenet (pretrained=True)\n",
            "  Replaced final layer. Original input features: 1024, New output classes: 10\n",
            "\n",
            "--- Phase 1: Feature Extraction ---\n",
            "Freezing base model parameters for feature extraction.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/googlenet.py:341: UserWarning: auxiliary heads in the pretrained googlenet model are NOT pretrained, so make sure to train them\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 1.8057 Acc: 0.7218\n",
            "Val Loss: 0.6345 Acc: 0.7846\n",
            "  New best validation accuracy: 0.7846\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 1.5712 Acc: 0.7607\n",
            "Val Loss: 0.5966 Acc: 0.7962\n",
            "  New best validation accuracy: 0.7962\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 1.5477 Acc: 0.7652\n",
            "Val Loss: 0.5683 Acc: 0.8045\n",
            "  New best validation accuracy: 0.8045\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 1.5507 Acc: 0.7648\n",
            "Val Loss: 0.5698 Acc: 0.8021\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 1.5403 Acc: 0.7693\n",
            "Val Loss: 0.5748 Acc: 0.8048\n",
            "  New best validation accuracy: 0.8048\n",
            "\n",
            "Training complete in 4m 39s\n",
            "Best val Acc: 0.804800\n",
            "\n",
            "--- Phase 2: Fine-Tuning ---\n",
            "Unfreezing all model parameters for fine-tuning.\n",
            "Epoch 1/10\n",
            "----------\n",
            "Train Loss: 1.1462 Acc: 0.8528\n",
            "Val Loss: 0.2755 Acc: 0.9044\n",
            "  New best validation accuracy: 0.9044\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train Loss: 0.8425 Acc: 0.9004\n",
            "Val Loss: 0.2168 Acc: 0.9265\n",
            "  New best validation accuracy: 0.9265\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train Loss: 0.7083 Acc: 0.9207\n",
            "Val Loss: 0.1938 Acc: 0.9346\n",
            "  New best validation accuracy: 0.9346\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train Loss: 0.6216 Acc: 0.9330\n",
            "Val Loss: 0.1732 Acc: 0.9399\n",
            "  New best validation accuracy: 0.9399\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train Loss: 0.5593 Acc: 0.9421\n",
            "Val Loss: 0.1603 Acc: 0.9428\n",
            "  New best validation accuracy: 0.9428\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train Loss: 0.5061 Acc: 0.9502\n",
            "Val Loss: 0.1566 Acc: 0.9451\n",
            "  New best validation accuracy: 0.9451\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train Loss: 0.4682 Acc: 0.9555\n",
            "Val Loss: 0.1559 Acc: 0.9485\n",
            "  New best validation accuracy: 0.9485\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train Loss: 0.4377 Acc: 0.9604\n",
            "Val Loss: 0.1502 Acc: 0.9510\n",
            "  New best validation accuracy: 0.9510\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train Loss: 0.4055 Acc: 0.9657\n",
            "Val Loss: 0.1530 Acc: 0.9501\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train Loss: 0.3809 Acc: 0.9686\n",
            "Val Loss: 0.1495 Acc: 0.9517\n",
            "  New best validation accuracy: 0.9517\n",
            "\n",
            "Training complete in 13m 4s\n",
            "Best val Acc: 0.951700\n",
            "\n",
            "Evaluating on test set...\n",
            "Evaluation complete.\n",
            "\n",
            "--- Results for googlenet_CIFAR10 ---\n",
            "Final Test Set Metrics:\n",
            "  Accuracy: 0.9517\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configuration and Hyperparameters\n",
        "NUM_CLASSES = 10\n",
        "DATASET_LIST = [\"MNIST\", \"CIFAR10\"]\n",
        "DATA_DIR = './data'\n",
        "MODEL_LIST = [\"vgg16\", \"resnet18\", \"googlenet\"]\n",
        "INPUT_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_FEATURE_EXTRACT = 5\n",
        "EPOCHS_FINE_TUNE = 10\n",
        "LR_CLASSIFIER = 1e-3\n",
        "LR_FINE_TUNE_BASE = 1e-5\n",
        "OPTIMIZER_TYPE = \"Adam\"\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs('./plots', exist_ok=True)\n",
        "\n",
        "# Data Handling Functions\n",
        "def get_transforms(dataset_name, input_size, is_train=True):\n",
        "    transform_list = [transforms.Resize((input_size, input_size))]\n",
        "    if dataset_name == \"MNIST\":\n",
        "        transform_list.append(transforms.Grayscale(num_output_channels=3))\n",
        "    if is_train and dataset_name == \"CIFAR10\":\n",
        "        transform_list.extend([transforms.RandomHorizontalFlip(), transforms.RandomCrop(input_size, padding=4)])\n",
        "    transform_list.extend([transforms.ToTensor(), transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "def get_dataloaders(dataset_name, batch_size, input_size, data_dir=DATA_DIR):\n",
        "    train_transform = get_transforms(dataset_name, input_size, is_train=True)\n",
        "    val_transform = get_transforms(dataset_name, input_size, is_train=False)\n",
        "    if dataset_name == \"MNIST\":\n",
        "        train_dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=train_transform)\n",
        "        test_dataset = datasets.MNIST(root=data_dir, train=False, download=True, transform=val_transform)\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_transform)\n",
        "        test_dataset = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=val_transform)\n",
        "        class_names = test_dataset.classes\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    dataloaders = {'train': train_loader, 'val': test_loader}\n",
        "    dataset_sizes = {'train': len(train_dataset), 'val': len(test_dataset)}\n",
        "    print(f\"Dataset: {dataset_name}\\n  Training samples: {dataset_sizes['train']}\\n  Validation/Test samples: {dataset_sizes['val']}\\n  Number of classes: {len(class_names)}\")\n",
        "    return dataloaders, dataset_sizes, class_names\n",
        "\n",
        "# Model Adaptation Functions\n",
        "def get_model(model_name, num_classes, pretrained=True):\n",
        "    print(f\"Loading model: {model_name} (pretrained={pretrained})\")\n",
        "    if model_name == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=pretrained)\n",
        "        input_features = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(input_features, num_classes)\n",
        "    elif model_name == \"resnet18\":\n",
        "        model = models.resnet18(pretrained=pretrained)\n",
        "        input_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(input_features, num_classes)\n",
        "    elif model_name == \"googlenet\":\n",
        "        model = models.googlenet(pretrained=pretrained, aux_logits=True)\n",
        "        input_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(input_features, num_classes)\n",
        "        if model.aux1 is not None:\n",
        "            input_features_aux1 = model.aux1.fc2.in_features\n",
        "            model.aux1.fc2 = nn.Linear(input_features_aux1, num_classes)\n",
        "        if model.aux2 is not None:\n",
        "            input_features_aux2 = model.aux2.fc2.in_features\n",
        "            model.aux2.fc2 = nn.Linear(input_features_aux2, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
        "    print(f\"  Replaced final layer. Original input features: {input_features}, New output classes: {num_classes}\")\n",
        "    return model\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        print(\"Freezing base model parameters for feature extraction.\")\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        print(\"Unfreezing all model parameters for fine-tuning.\")\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, device, num_epochs=10):\n",
        "    start_time = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}\\n{\"-\" * 10}')\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss, running_corrects = 0.0, 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    if hasattr(outputs, 'logits'):  # For GoogLeNet\n",
        "                        loss = criterion(outputs.logits, labels)\n",
        "                        if phase == 'train':\n",
        "                            loss += 0.3 * criterion(outputs.aux_logits2, labels)\n",
        "                            loss += 0.3 * criterion(outputs.aux_logits1, labels)\n",
        "                        _, preds = torch.max(outputs.logits, 1)\n",
        "                    else:\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                print(f'  New best validation accuracy: {best_acc:.4f}')\n",
        "        print()\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(model, dataloader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            if hasattr(outputs, 'logits'):  # For GoogLeNet\n",
        "                outputs = outputs.logits\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    class_report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0, output_dict=True)\n",
        "    print(\"Evaluation complete.\")\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'classification_report': class_report\n",
        "    }\n",
        "\n",
        "# Visualization Functions\n",
        "def plot_dataset_samples(dataloader, class_names, dataset_name):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < len(images):\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            img = img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
        "            img = np.clip(img, 0, 1)\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(class_names[labels[i]])\n",
        "            ax.axis('off')\n",
        "    plt.suptitle(f'Sample Images from {dataset_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./plots/dataset_samples_{dataset_name.lower()}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_combined_history(history_fe, history_ft, dataset_name, model_name):\n",
        "    train_loss = history_fe['train_loss'] + history_ft['train_loss']\n",
        "    val_loss = history_fe['val_loss'] + history_ft['val_loss']\n",
        "    train_acc = history_fe['train_acc'] + history_ft['train_acc']\n",
        "    val_acc = history_fe['val_acc'] + history_ft['val_acc']\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "    plt.axvline(x=EPOCHS_FEATURE_EXTRACT, color='k', linestyle='--', label='Feature Extraction End')\n",
        "    plt.title(f'Training and Validation Accuracy for {model_name} on {dataset_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'./plots/{dataset_name.lower()}_accuracy_curves_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "    plt.axvline(x=EPOCHS_FEATURE_EXTRACT, color='k', linestyle='--', label='Feature Extraction End')\n",
        "    plt.title(f'Training and Validation Loss for {model_name} on {dataset_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'./plots/{dataset_name.lower()}_loss_curves_{model_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def show_sample_predictions(model, dataloader, class_names, device, experiment_name):\n",
        "    model.eval()\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        if hasattr(outputs, 'logits'):  # For GoogLeNet\n",
        "            outputs = outputs.logits\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = img * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)  # Denormalize\n",
        "        img = np.clip(img, 0, 1)\n",
        "        ax.imshow(img)\n",
        "        true_label = class_names[labels[i]]\n",
        "        pred_label = class_names[preds[i]]\n",
        "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color='green' if true_label == pred_label else 'red')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./plots/{experiment_name}_sample_predictions.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names, experiment_name):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix for {experiment_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(f'./plots/confusion_matrix_{experiment_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_model_comparison(results, dataset_name):\n",
        "    models = list(results.keys())\n",
        "    accuracies = [results[model]['accuracy'] for model in models]\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(models, accuracies, color='skyblue')\n",
        "    plt.title(f'Model Performance Comparison on {dataset_name}')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(accuracies):\n",
        "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
        "    plt.savefig(f'./plots/model_comparison_{dataset_name.lower()}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Main Execution Block\n",
        "if __name__ == \"__main__\":\n",
        "    for dataset_name in DATASET_LIST:\n",
        "        dataloaders, dataset_sizes, class_names = get_dataloaders(dataset_name, BATCH_SIZE, INPUT_SIZE)\n",
        "        plot_dataset_samples(dataloaders['train'], class_names, dataset_name)\n",
        "        results = {}\n",
        "        for model_name in MODEL_LIST:\n",
        "            experiment_name = f\"{model_name}_{dataset_name}\"\n",
        "            print(f\"\\n{'='*20} Starting Experiment: {experiment_name} {'='*20}\")\n",
        "            model = get_model(model_name, NUM_CLASSES, pretrained=True).to(device)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Feature Extraction Phase\n",
        "            print(\"\\n--- Phase 1: Feature Extraction ---\")\n",
        "            set_parameter_requires_grad(model, feature_extracting=True)\n",
        "            if model_name == \"vgg16\":\n",
        "                for param in model.classifier[6].parameters():\n",
        "                    param.requires_grad = True\n",
        "                params_to_update_fe = list(model.classifier[6].parameters())\n",
        "            elif model_name == \"resnet18\":\n",
        "                for param in model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "                params_to_update_fe = list(model.fc.parameters())\n",
        "            elif model_name == \"googlenet\":\n",
        "                for param in model.fc.parameters():\n",
        "                    param.requires_grad = True\n",
        "                if model.aux1 is not None:\n",
        "                    for param in model.aux1.fc2.parameters():\n",
        "                        param.requires_grad = True\n",
        "                if model.aux2 is not None:\n",
        "                    for param in model.aux2.fc2.parameters():\n",
        "                        param.requires_grad = True\n",
        "                params_to_update_fe = list(model.fc.parameters())\n",
        "                if model.aux1 is not None:\n",
        "                    params_to_update_fe.extend(model.aux1.fc2.parameters())\n",
        "                if model.aux2 is not None:\n",
        "                    params_to_update_fe.extend(model.aux2.fc2.parameters())\n",
        "            optimizer_fe = optim.Adam(params_to_update_fe, lr=LR_CLASSIFIER) if OPTIMIZER_TYPE == \"Adam\" else optim.SGD(params_to_update_fe, lr=LR_CLASSIFIER)\n",
        "            model_fe, history_fe = train_model(model, dataloaders, dataset_sizes, criterion, optimizer_fe, device, EPOCHS_FEATURE_EXTRACT)\n",
        "\n",
        "            # Fine-Tuning Phase\n",
        "            print(\"\\n--- Phase 2: Fine-Tuning ---\")\n",
        "            set_parameter_requires_grad(model, feature_extracting=False)\n",
        "            if model_name == \"vgg16\":\n",
        "                base_params = list(model.features.parameters())\n",
        "                classifier_params = list(model.classifier[6].parameters())\n",
        "            elif model_name == \"resnet18\":\n",
        "                base_params = [p for p in model.parameters() if p not in set(model.fc.parameters())]\n",
        "                classifier_params = list(model.fc.parameters())\n",
        "            elif model_name == \"googlenet\":\n",
        "                classifier_params = list(model.fc.parameters())\n",
        "                if model.aux1 is not None:\n",
        "                    classifier_params.extend(model.aux1.fc2.parameters())\n",
        "                if model.aux2 is not None:\n",
        "                    classifier_params.extend(model.aux2.fc2.parameters())\n",
        "                base_params = [p for p in model.parameters() if p not in set(classifier_params)]\n",
        "            params_to_update_ft = [{'params': base_params, 'lr': LR_FINE_TUNE_BASE}, {'params': classifier_params, 'lr': LR_CLASSIFIER}]\n",
        "            optimizer_ft = optim.Adam(params_to_update_ft) if OPTIMIZER_TYPE == \"Adam\" else optim.SGD(params_to_update_ft)\n",
        "            model_ft, history_ft = train_model(model, dataloaders, dataset_sizes, criterion, optimizer_ft, device, EPOCHS_FINE_TUNE)\n",
        "\n",
        "            # Evaluation\n",
        "            final_metrics = evaluate_model(model_ft, dataloaders['val'], device, class_names)\n",
        "            results[model_name] = final_metrics\n",
        "            print(f\"\\n--- Results for {experiment_name} ---\\nFinal Test Set Metrics:\\n  Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "\n",
        "            # Visualizations\n",
        "            plot_combined_history(history_fe, history_ft, dataset_name, model_name)\n",
        "            show_sample_predictions(model_ft, dataloaders['val'], class_names, device, experiment_name)\n",
        "            if model_name == \"resnet18\" and dataset_name == \"CIFAR10\":\n",
        "                plot_confusion_matrix(final_metrics['confusion_matrix'], class_names, experiment_name)\n",
        "\n",
        "        # Model Comparison\n",
        "        plot_model_comparison(results, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SzyNw5GcFZ"
      },
      "source": [
        "## Parallel Model Code\n",
        "This script is dedicated to training and evaluating parallel architectures, where two models are combined to extract features, which are then concatenated and passed through a final classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uboDfPlSia2n"
      },
      "source": [
        "1. Imports and Device Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ahhrqywiaYZ",
        "outputId": "ca166e64-22e5-4234-bb31-9afed4fafc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-gzTu95igCp"
      },
      "source": [
        "2. Data Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4bpwAJBsijAt"
      },
      "outputs": [],
      "source": [
        "# Data transformations\n",
        "transform_mnist = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to 3 channels for pretrained models\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])\n",
        "\n",
        "transform_cifar10 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oMv_CUIimuJ"
      },
      "source": [
        "3. Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPEWY2kFip2N",
        "outputId": "728fc2f5-c680-4283-cf77-d336290c59be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "MNIST - Training: 60000 images, Test: 10000 images\n",
            "CIFAR10 - Training: 50000 images, Test: 10000 images\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
        "\n",
        "# CIFAR10 dataset\n",
        "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar10)\n",
        "cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar10)\n",
        "\n",
        "print(f\"MNIST - Training: {len(mnist_train)} images, Test: {len(mnist_test)} images\")\n",
        "print(f\"CIFAR10 - Training: {len(cifar10_train)} images, Test: {len(cifar10_test)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4bKSqzCisgc"
      },
      "source": [
        "4. Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CGobEnJ3iwVj"
      },
      "outputs": [],
      "source": [
        "# Data loaders\n",
        "batch_size = 64\n",
        "mnist_train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "cifar10_train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True)\n",
        "cifar10_test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFydw0tkiy08"
      },
      "source": [
        "5. Feature Extractor Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZTbiUEIWi14D"
      },
      "outputs": [],
      "source": [
        "# Feature extractor classes\n",
        "class ResNet18Features(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(ResNet18Features, self).__init__()\n",
        "        self.features = nn.Sequential(*list(original_model.children())[:-2])  # Up to avgpool\n",
        "        self.avgpool = original_model.avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "class VGG16Features(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        super(VGG16Features, self).__init__()\n",
        "        self.features = original_model.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "class GoogleNetFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogleNetFeatures, self).__init__()\n",
        "        self.model = models.googlenet(pretrained=True)\n",
        "        self.model.fc = nn.Identity()  # Replace fc with identity\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i60IZT7Oi5cI"
      },
      "source": [
        "6. Parallel Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PUZaq6tli8q6"
      },
      "outputs": [],
      "source": [
        "# Parallel Model\n",
        "class ParallelModel(nn.Module):\n",
        "    def __init__(self, feature_extractor1, feature_extractor2, model_name1, model_name2, num_classes=10):\n",
        "        super(ParallelModel, self).__init__()\n",
        "        self.feature_extractor1 = feature_extractor1\n",
        "        self.feature_extractor2 = feature_extractor2\n",
        "        feature_sizes = {'resnet18': 512, 'vgg16': 512, 'googlenet': 1024}\n",
        "        feature_size1 = feature_sizes[model_name1]\n",
        "        feature_size2 = feature_sizes[model_name2]\n",
        "        self.classifier = nn.Linear(feature_size1 + feature_size2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features1 = self.feature_extractor1(x)\n",
        "        features2 = self.feature_extractor2(x)\n",
        "        combined_features = torch.cat((features1, features2), dim=1)\n",
        "        output = self.classifier(combined_features)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF2m7oKzjBVT"
      },
      "source": [
        "7. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fWJDv43ijENa"
      },
      "outputs": [],
      "source": [
        "# Helper function to get feature extractors\n",
        "def get_feature_extractor(model_name):\n",
        "    if model_name == 'resnet18':\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        return ResNet18Features(model).to(device)\n",
        "    elif model_name == 'vgg16':\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        return VGG16Features(model).to(device)\n",
        "    elif model_name == 'googlenet':\n",
        "        return GoogleNetFeatures().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBgWQmnNjHsY"
      },
      "source": [
        "8. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oCuSWgy9jL8s"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation functions\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def compute_confusion_matrix(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    cm = confusion_matrix(all_targets, all_preds)\n",
        "    return cm\n",
        "\n",
        "def train_parallel_model(parallel_model, model_name1, model_name2, train_loader, test_loader, num_epochs, base_lr, classifier_lr):\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': parallel_model.feature_extractor1.parameters(), 'lr': base_lr},\n",
        "        {'params': parallel_model.feature_extractor2.parameters(), 'lr': base_lr},\n",
        "        {'params': parallel_model.classifier.parameters(), 'lr': classifier_lr}\n",
        "    ])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"Training for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        loss, train_acc = train_one_epoch(parallel_model, train_loader, optimizer, criterion)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}: Loss={loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "    test_acc = evaluate(parallel_model, test_loader)\n",
        "    print(f'Final Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "    cm = compute_confusion_matrix(parallel_model, test_loader)\n",
        "    print(f'Confusion Matrix for Parallel {model_name1}+{model_name2}:\\n{cm}')\n",
        "\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3CqOjhHjQUC"
      },
      "source": [
        "9. Main Execution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgCSR65IjUjH",
        "outputId": "e2fb3c8c-bd71-4428-e586-c0dbb8ad0d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training parallel models on MNIST ===\n",
            "\n",
            "\n",
            "Training parallel model: resnet18+vgg16\n",
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.0568, Train Acc=98.31%\n",
            "Epoch 2/10: Loss=0.0216, Train Acc=99.35%\n",
            "Epoch 3/10: Loss=0.0172, Train Acc=99.47%\n",
            "Epoch 4/10: Loss=0.0140, Train Acc=99.57%\n",
            "Epoch 5/10: Loss=0.0106, Train Acc=99.68%\n",
            "Epoch 6/10: Loss=0.0087, Train Acc=99.71%\n",
            "Epoch 7/10: Loss=0.0118, Train Acc=99.64%\n",
            "Epoch 8/10: Loss=0.0079, Train Acc=99.75%\n",
            "Epoch 9/10: Loss=0.0055, Train Acc=99.83%\n",
            "Epoch 10/10: Loss=0.0078, Train Acc=99.76%\n",
            "Final Test Accuracy: 99.41%\n",
            "Confusion Matrix for Parallel resnet18+vgg16:\n",
            "[[ 979    0    0    0    0    0    1    0    0    0]\n",
            " [   5 1123    0    2    0    0    1    4    0    0]\n",
            " [   1    0 1028    0    0    0    0    3    0    0]\n",
            " [   0    0    1 1009    0    0    0    0    0    0]\n",
            " [   0    0    0    0  980    0    0    0    0    2]\n",
            " [   0    0    0   12    0  879    1    0    0    0]\n",
            " [   1    2    0    0    0    0  953    0    2    0]\n",
            " [   0    2    1    1    0    0    0 1024    0    0]\n",
            " [   3    0    1    0    0    1    0    0  967    2]\n",
            " [   0    0    0    0    9    1    0    0    0  999]]\n",
            "\n",
            "\n",
            "Training parallel model: resnet18+googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.0593, Train Acc=98.31%\n",
            "Epoch 2/10: Loss=0.0224, Train Acc=99.33%\n",
            "Epoch 3/10: Loss=0.0180, Train Acc=99.47%\n",
            "Epoch 4/10: Loss=0.0147, Train Acc=99.56%\n",
            "Epoch 5/10: Loss=0.0134, Train Acc=99.57%\n",
            "Epoch 6/10: Loss=0.0136, Train Acc=99.59%\n",
            "Epoch 7/10: Loss=0.0114, Train Acc=99.67%\n",
            "Epoch 8/10: Loss=0.0108, Train Acc=99.70%\n",
            "Epoch 9/10: Loss=0.0066, Train Acc=99.81%\n",
            "Epoch 10/10: Loss=0.0093, Train Acc=99.75%\n",
            "Final Test Accuracy: 99.55%\n",
            "Confusion Matrix for Parallel resnet18+googlenet:\n",
            "[[ 979    0    0    0    0    0    1    0    0    0]\n",
            " [   0 1133    1    0    0    0    0    1    0    0]\n",
            " [   1    0 1029    0    0    0    0    2    0    0]\n",
            " [   0    0    0 1008    0    1    0    0    1    0]\n",
            " [   0    0    0    0  972    0    0    0    0   10]\n",
            " [   0    0    0    3    0  886    0    0    1    2]\n",
            " [   4    1    0    0    0    1  946    0    5    1]\n",
            " [   0    3    2    0    1    0    0 1021    0    1]\n",
            " [   0    0    0    0    0    0    0    0  973    1]\n",
            " [   0    0    0    0    1    0    0    0    0 1008]]\n",
            "\n",
            "\n",
            "Training parallel model: vgg16+googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.0673, Train Acc=97.91%\n",
            "Epoch 2/10: Loss=0.0219, Train Acc=99.33%\n",
            "Epoch 3/10: Loss=0.0146, Train Acc=99.57%\n",
            "Epoch 4/10: Loss=0.0143, Train Acc=99.53%\n",
            "Epoch 5/10: Loss=0.0099, Train Acc=99.69%\n",
            "Epoch 6/10: Loss=0.0120, Train Acc=99.65%\n",
            "Epoch 7/10: Loss=0.0109, Train Acc=99.67%\n",
            "Epoch 8/10: Loss=0.0072, Train Acc=99.79%\n",
            "Epoch 9/10: Loss=0.0068, Train Acc=99.78%\n",
            "Epoch 10/10: Loss=0.0081, Train Acc=99.75%\n",
            "Final Test Accuracy: 99.51%\n",
            "Confusion Matrix for Parallel vgg16+googlenet:\n",
            "[[ 980    0    0    0    0    0    0    0    0    0]\n",
            " [   0 1135    0    0    0    0    0    0    0    0]\n",
            " [   0    1 1030    0    0    0    0    1    0    0]\n",
            " [   0    0    0 1005    0    5    0    0    0    0]\n",
            " [   0    0    0    0  975    0    0    0    0    7]\n",
            " [   0    0    0    1    0  891    0    0    0    0]\n",
            " [   4    3    0    0    1    6  943    0    1    0]\n",
            " [   0    4    4    0    0    0    0 1020    0    0]\n",
            " [   0    0    1    2    0    0    0    0  971    0]\n",
            " [   0    0    0    0    4    0    0    2    2 1001]]\n",
            "\n",
            "=== Results Summary for MNIST ===\n",
            "resnet18+vgg16: 99.41%\n",
            "resnet18+googlenet: 99.55%\n",
            "vgg16+googlenet: 99.51%\n",
            "\n",
            "=== Training parallel models on CIFAR-10 ===\n",
            "\n",
            "\n",
            "Training parallel model: resnet18+vgg16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.3111, Train Acc=89.43%\n",
            "Epoch 2/10: Loss=0.1026, Train Acc=96.42%\n",
            "Epoch 3/10: Loss=0.0583, Train Acc=97.99%\n",
            "Epoch 4/10: Loss=0.0463, Train Acc=98.46%\n",
            "Epoch 5/10: Loss=0.0440, Train Acc=98.53%\n",
            "Epoch 6/10: Loss=0.0335, Train Acc=98.87%\n",
            "Epoch 7/10: Loss=0.0323, Train Acc=98.81%\n",
            "Epoch 8/10: Loss=0.0347, Train Acc=98.85%\n",
            "Epoch 9/10: Loss=0.0321, Train Acc=98.93%\n",
            "Epoch 10/10: Loss=0.0240, Train Acc=99.18%\n",
            "Final Test Accuracy: 93.98%\n",
            "Confusion Matrix for Parallel resnet18+vgg16:\n",
            "[[950   2   7   3   5   1   1   1  14  16]\n",
            " [  6 967   0   0   0   0   1   0   6  20]\n",
            " [ 15   0 928  12  10  10  21   2   2   0]\n",
            " [  7   1  14 873  21  47  19  12   2   4]\n",
            " [  4   0   6  13 951   6   7  12   0   1]\n",
            " [  1   0  11  74  12 879   8  15   0   0]\n",
            " [  5   1   4   9   1   2 975   2   1   0]\n",
            " [  5   0   4   4  16  25   1 943   0   2]\n",
            " [ 21   4   3   5   0   0   2   0 958   7]\n",
            " [  3  14   0   2   1   0   0   0   6 974]]\n",
            "\n",
            "\n",
            "Training parallel model: resnet18+googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.2853, Train Acc=90.45%\n",
            "Epoch 2/10: Loss=0.0910, Train Acc=96.97%\n",
            "Epoch 3/10: Loss=0.0547, Train Acc=98.10%\n",
            "Epoch 4/10: Loss=0.0463, Train Acc=98.40%\n",
            "Epoch 5/10: Loss=0.0482, Train Acc=98.41%\n",
            "Epoch 6/10: Loss=0.0428, Train Acc=98.58%\n",
            "Epoch 7/10: Loss=0.0367, Train Acc=98.78%\n",
            "Epoch 8/10: Loss=0.0311, Train Acc=98.95%\n",
            "Epoch 9/10: Loss=0.0352, Train Acc=98.85%\n",
            "Epoch 10/10: Loss=0.0295, Train Acc=99.08%\n",
            "Final Test Accuracy: 94.94%\n",
            "Confusion Matrix for Parallel resnet18+googlenet:\n",
            "[[938   2  15   7   3   0   4   0  19  12]\n",
            " [  0 993   0   1   0   0   0   0   2   4]\n",
            " [  1   0 952  14  11  10   8   2   2   0]\n",
            " [  2   2   9 902   9  56  11   3   4   2]\n",
            " [  2   0   4  13 967   4   0  10   0   0]\n",
            " [  0   0   9  42  16 924   2   6   0   1]\n",
            " [  2   0  13  15   6   2 962   0   0   0]\n",
            " [  2   0   5   9  17   6   0 959   0   2]\n",
            " [ 17   7   4   1   0   0   1   0 950  20]\n",
            " [  2  44   1   2   0   0   0   0   4 947]]\n",
            "\n",
            "\n",
            "Training parallel model: vgg16+googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10 epochs...\n",
            "Epoch 1/10: Loss=0.3081, Train Acc=89.64%\n",
            "Epoch 2/10: Loss=0.1006, Train Acc=96.61%\n",
            "Epoch 3/10: Loss=0.0568, Train Acc=98.09%\n",
            "Epoch 4/10: Loss=0.0425, Train Acc=98.49%\n",
            "Epoch 5/10: Loss=0.0371, Train Acc=98.74%\n",
            "Epoch 6/10: Loss=0.0364, Train Acc=98.72%\n",
            "Epoch 7/10: Loss=0.0255, Train Acc=99.14%\n",
            "Epoch 8/10: Loss=0.0296, Train Acc=99.01%\n",
            "Epoch 9/10: Loss=0.0229, Train Acc=99.26%\n",
            "Epoch 10/10: Loss=0.0234, Train Acc=99.22%\n",
            "Final Test Accuracy: 94.39%\n",
            "Confusion Matrix for Parallel vgg16+googlenet:\n",
            "[[960   2  10   3   0   0   0   1  22   2]\n",
            " [  1 987   1   0   0   0   0   0   2   9]\n",
            " [  6   0 942   8  11  17   8   6   2   0]\n",
            " [  5   1  17 821  18 103  11  21   2   1]\n",
            " [  5   0  11   6 952   5   1  19   1   0]\n",
            " [  1   0   4  30  13 934   1  16   0   1]\n",
            " [  3   0  15   8   6   7 961   0   0   0]\n",
            " [  0   0   5   3  11   8   1 972   0   0]\n",
            " [ 14   3   4   3   0   1   1   1 968   5]\n",
            " [  7  40   0   1   0   1   0   1   8 942]]\n",
            "\n",
            "=== Results Summary for CIFAR-10 ===\n",
            "resnet18+vgg16: 93.98%\n",
            "resnet18+googlenet: 94.94%\n",
            "vgg16+googlenet: 94.39%\n",
            "\n",
            "=== Comparison Across Datasets ===\n",
            "resnet18+vgg16: MNIST = 99.41%, CIFAR-10 = 93.98%\n",
            "resnet18+googlenet: MNIST = 99.55%, CIFAR-10 = 94.94%\n",
            "vgg16+googlenet: MNIST = 99.51%, CIFAR-10 = 94.39%\n"
          ]
        }
      ],
      "source": [
        "# For notebook environment - Now runs on both datasets\n",
        "datasets = ['mnist', 'cifar10']\n",
        "all_results = {}  # Dictionary to store results for all datasets\n",
        "\n",
        "for dataset in datasets:\n",
        "    # Select dataset\n",
        "    if dataset == 'mnist':\n",
        "        train_loader = mnist_train_loader\n",
        "        test_loader = mnist_test_loader\n",
        "        dataset_name = 'MNIST'\n",
        "    else:\n",
        "        train_loader = cifar10_train_loader\n",
        "        test_loader = cifar10_test_loader\n",
        "        dataset_name = 'CIFAR-10'\n",
        "\n",
        "    print(f'\\n=== Training parallel models on {dataset_name} ===')\n",
        "\n",
        "    # Define model pairs to train\n",
        "    pairs = [('resnet18', 'vgg16'), ('resnet18', 'googlenet'), ('vgg16', 'googlenet')]\n",
        "    dataset_results = {}  # Results for this specific dataset\n",
        "\n",
        "    # Train each model pair\n",
        "    for model1_name, model2_name in pairs:\n",
        "        print(f'\\n\\nTraining parallel model: {model1_name}+{model2_name}')\n",
        "        feature_extractor1 = get_feature_extractor(model1_name)\n",
        "        feature_extractor2 = get_feature_extractor(model2_name)\n",
        "        parallel_model = ParallelModel(feature_extractor1, feature_extractor2, model1_name, model2_name).to(device)\n",
        "\n",
        "        # Train model\n",
        "        acc = train_parallel_model(\n",
        "            parallel_model,\n",
        "            model1_name,\n",
        "            model2_name,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            num_epochs=10,  # Reduced from 10 for quicker execution\n",
        "            base_lr=1e-4,\n",
        "            classifier_lr=1e-3\n",
        "        )\n",
        "\n",
        "        # Store results for this dataset\n",
        "        dataset_results[f\"{model1_name}+{model2_name}\"] = acc\n",
        "\n",
        "    # Store results for this dataset in the overall results dictionary\n",
        "    all_results[dataset_name] = dataset_results\n",
        "\n",
        "    # Print summary of results for this dataset\n",
        "    print(f\"\\n=== Results Summary for {dataset_name} ===\")\n",
        "    for model_pair, accuracy in dataset_results.items():\n",
        "        print(f\"{model_pair}: {accuracy:.2f}%\")\n",
        "\n",
        "# Print comparison across datasets\n",
        "print(\"\\n=== Comparison Across Datasets ===\")\n",
        "for model_pair in [f\"{m1}+{m2}\" for m1, m2 in pairs]:\n",
        "    mnist_acc = all_results['MNIST'][model_pair]\n",
        "    cifar_acc = all_results['CIFAR-10'][model_pair]\n",
        "    print(f\"{model_pair}: MNIST = {mnist_acc:.2f}%, CIFAR-10 = {cifar_acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}